{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = []\n",
    "#Y = []\n",
    "#for path, subdirs, files in os.walk(os.path.join(os.getcwd(),'data')):\n",
    "#    for name in files:\n",
    "#        selected_dir = os.path.join(path, name)\n",
    "#        t = selected_dir.split(\"\\\\\")\n",
    "#        if '.txt' in t[-1]:\n",
    "#            with open(selected_dir, 'rb') as f:\n",
    "#                X.append(ViTokenizer.tokenize(str(f.read(), 'utf-16-le')))\n",
    "#                Y.append(t[-1][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_data.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('Y_data.pkl', 'rb') as f:\n",
    "    Y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load vietnamese stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a lô',\n",
       " 'a ha',\n",
       " 'ai',\n",
       " 'ai ai',\n",
       " 'ai nấy',\n",
       " 'ai đó',\n",
       " 'alô',\n",
       " 'amen',\n",
       " 'anh',\n",
       " 'anh ấy']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('vietnamese-stopwords.txt', 'rb') as f:\n",
    "    stopwords = str(f.read(),'utf-8').split('\\r\\n')\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(X, open('X_data.pkl', 'wb'))\n",
    "#pickle.dump(Y, open('Y_data.pkl', 'wb'))\n",
    "#len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.66, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Pipleline. LinerSVC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([('vect', CountVectorizer(stop_words=stopwords)),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', LinearSVC()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit data to pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['bao', 'bay', 'biến', 'biệt', 'bây', 'bõm', 'bảo', 'bất', 'bẩy', 'bập', 'bắt', 'bội', 'chao', 'chi', 'chia', 'chu', 'chui', 'chuẩn', 'chà', 'chành', 'chí', 'chót', 'chùn', 'chăn', 'chũn', 'chưng', 'chạnh', 'chả', 'chầm', 'chầy', 'chập', 'chắn', 'chẳng', 'chết', 'chốc', 'chừ', 'chừng', 'coi', 'cu', 'cá', 'câu', 'cóc', 'công', 'cạnh', 'cảm', 'cầu', 'cật', 'cắt', 'cổ', 'cụ', 'cục', 'cực', 'da', 'dà', 'dĩ', 'dưng', 'dần', 'dầu', 'dịp', 'dở', 'dụng', 'gian', 'giá', 'giác', 'giời', 'ha', 'hiện', 'hoàn', 'hèn', 'hình', 'hô', 'hầu', 'hậu', 'hẳn', 'hồ', 'hỗ', 'hội', 'hợp', 'hự', 'khói', 'khô', 'khăn', 'khắc', 'khẳng', 'kia', 'kiện', 'kê', 'kì', 'kìa', 'kỳ', 'lai', 'le', 'liên', 'liệt', 'loạt', 'luận', 'luật', 'luốt', 'lình', 'lí', 'lô', 'lý', 'lập', 'lẽ', 'lị', 'lự', 'lực', 'mày', 'mòi', 'mù', 'mạng', 'mấy', 'mẹ', 'mực', 'nghiễm', 'ngõ', 'ngăn', 'ngắt', 'ngộ', 'ngờ', 'nhiên', 'nhiêu', 'nhiệt', 'nhung', 'nhân', 'nhén', 'nhón', 'nhăng', 'nhược', 'nhỡ', 'nả', 'nỗi', 'nở', 'nức', 'oai', 'phi', 'phui', 'phàm', 'phù', 'phăn', 'phương', 'phỉ', 'quan', 'qui', 'quy', 'quyết', 'ren', 'riu', 'ríu', 'rón', 'rút', 'rốt', 'sa', 'sinh', 'song', 'sả', 'sẻ', 'sốt', 'sột', 'sở', 'sợ', 'sức', 'sử', 'ta', 'te', 'tha', 'than', 'thay', 'thi', 'thiên', 'thiết', 'thoảng', 'thành', 'thái', 'tháo', 'thân', 'thình', 'thúng', 'thương', 'thảo', 'thảy', 'thắng', 'thể', 'thỉnh', 'thị', 'thời', 'thục', 'thử', 'thực', 'tiên', 'tiếp', 'tiện', 'tiệt', 'toàn', 'toé', 'trung', 'tráo', 'trình', 'trí', 'trạng', 'trạo', 'trếu', 'trệu', 'trị', 'trọi', 'trọng', 'trời', 'trở', 'trợ', 'trừ', 'trực', 'tuyệt', 'tuần', 'tuốt', 'tuồn', 'tuồng', 'tuột', 'tà', 'tàn', 'tán', 'tâm', 'tê', 'tì', 'tình', 'tít', 'tò', 'tông', 'tù', 'tăm', 'tả', 'tất', 'tần', 'tập', 'tật', 'tế', 'tề', 'tỏ', 'tốc', 'tối', 'tục', 'tức', 'tử', 'tựu', 'vung', 'vàn', 'ví', 'vô', 'văng', 'vạn', 'vả', 'vấn', 'vẻ', 'vị', 'vốn', 'xiết', 'xon', 'xoành', 'xoạch', 'xuất', 'xón', 'xúi', 'xăm', 'xưa', 'xả', 'xảy', 'xắm', 'xềnh', 'xệch', 'xử', 'xửa', 'yêu', 'âu', 'ôi', 'đi', 'đành', 'đán', 'đánh', 'đáo', 'đùng', 'đơn', 'đạch', 'đại', 'đảm', 'đất', 'đấy', 'đầu', 'đặc', 'đề', 'địa', 'định', 'đối', 'đồ', 'đồng', 'đổi', 'ơn', 'ầu', 'ối'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=['a lô', 'a...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate percent accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9217511885895404"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = clf.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
